# Chapter 4

__4.1__ Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.

|       |pos  |neg  |
|-------|-----|-----|
|I      |0.09 |0.16 |
|always |0.07 |0.06 |
|like   |0.29 |0.06 |
|foreign|0.04 |0.15 |
|films  |0.08 |0.11 |

What class will Naive bayes assign to the sentence `I always like foreign films.`?<br><br>

$P$(`I always like foreign films.`|pos) $= 0.09 * 0.07 * 0.29 * 0.04 * 0.08 = 0.0000058464$

$P$(`I always like foreign films.`|neg) $= 0.16 * 0.06 * 0.06 * 0.15 * 0.11 = 0.000009504$

Naive bayes assigns 'neg' class to the sentence `I always like foreign films.`.

---

__4.2__  Given the following short movie reviews, each labeled with a genre, either comedy or action:

1. fun, couple, love, love ->       __comedy__
2. fast, furious, shoot ->          __action__
3. couple, fly, fast, fun, fun ->   __comedy__
4. furious, shoot, shoot, fun ->    __action__
5. fly, fast, shoot, love ->        __action__

and a new document $D$:

- fast, couple, shoot, fly

compute the most likely class for $D$. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods.<br><br>

First we compute $P(c)$:

$P(\text{comedy}) = \frac{2}{5}$

$P(\text{action}) = \frac{3}{5}$

Then we compute the probabilities of each word in new document $D$ given class $c$ using add-1 smoothing:

$P(\text{fast}|\text{comedy}) = \frac{1 + 1}{9 + 7} = \frac{2}{16} = \frac{1}{8}$

$P(\text{fast}|\text{action}) = \frac{2 + 1}{11 + 7} = \frac{3}{18} = \frac{1}{6}$


$P(\text{couple}|\text{comedy}) = \frac{2 + 1}{9 + 7} = \frac{3}{16}$

$P(\text{couple}|\text{action}) = \frac{0 + 1}{11 + 7} = \frac{1}{18}$


$P(\text{shoot}|\text{comedy}) = \frac{0 + 1}{9 + 7} = \frac{1}{16}$

$P(\text{shoot}|\text{action}) = \frac{4 + 1}{11 + 7} = \frac{5}{18}$


$P(\text{fly}|\text{comedy}) = \frac{1 + 1}{9 + 7} = \frac{2}{16} = \frac{1}{8}$

$P(\text{fly}|\text{action}) = \frac{1 + 1}{11 + 7} = \frac{2}{18} = \frac{1}{9}$

Now we compute the likeihood of $D$ being generated by each of the 2 classes:

$P(\text{comedy}) \cdot P(D|\text{comedy})$
$= P(\text{comedy}) \cdot P(\text{fast}|\text{comedy}) \cdot P(\text{couple}|\text{comedy}) \cdot P(\text{shoot}|\text{comedy}) \cdot P(\text{fly}|\text{comedy})$

$= \frac{2}{5} \cdot \frac{1}{8} \cdot \frac{3}{16} \cdot \frac{1}{16} \cdot \frac{1}{8}$
$= 7.3242\times10^{-5}$

$P(\text{action}) \cdot P(D|\text{action})$
$= P(\text{action}) \cdot P(\text{fast}|\text{action}) \cdot P(\text{couple}|\text{action}) \cdot P(\text{shoot}|\text{action}) \cdot P(\text{fly}|\text{action})$

$= \frac{3}{5} \cdot \frac{1}{6} \cdot \frac{1}{18} \cdot \frac{5}{18} \cdot \frac{1}{9}$
$= 1.7146\times10^{-4}$

The most likeily class for $D$ is the class 'action'.

---

__4.3__ Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.

|doc  |“good” |“poor” |“great”|(class)|
|-----|-------|-------|-------|-------|
|d1.  |3      |0      |3      |pos    |
|d2.  |0      |1      |2      |pos    |
|d3.  |1      |3      |0      |neg    |
|d4.  |1      |5      |2      |neg    |
|d5.  |0      |2      |0      |neg    |

Use both naive Bayes models to assign a class (pos or neg) to this sentence: 

A good, good plot and great characters, but poor acting.

Recall from page 62 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don’t throw out words that appear in some classes but not others; that’s what add-one smoothing is for.) Do the two models agree or disagree?<br><br>

Print-out from naive-bayes.py:

```
mode:                       multinomial
-------------------------------------------------------
logprior for "pos":         -0.39794000867203755
logprior for "neg":         -0.22184874961635637
loglikelihood for "pos":    {'good': -0.47712125471966244, 'poor': -0.7781512503836435, 'great': -0.30102999566398114}
loglikelihood for "neg":    {'good': -0.7533276666586114, 'poor': -0.18905623622004886, 'great': -0.7533276666586114}
sum for "pos" class:        -2.431363764158987
sum for "neg" class:        -2.6708879858122394
most likely class:          pos

mode:                       binary
-------------------------------------------------------
logprior for "pos":         -0.39794000867203755
logprior for "neg":         -0.22184874961635637
loglikelihood for "pos":    {'good': -0.7781512503836435, 'poor': -0.7781512503836435, 'great': -0.6020599913279623}
loglikelihood for "neg":    {'good': -0.7533276666586114, 'poor': -0.6283889300503115, 'great': -0.9294189257142926}
sum for "pos" class:        -3.3344537511509302
sum for "neg" class:        -3.2863119386981836
most likely class:          neg
```

The two models does not agree. 
